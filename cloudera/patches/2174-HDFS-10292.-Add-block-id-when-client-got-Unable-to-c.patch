From 2bfaa8011f508a1320ffeb14e248c599dc5af838 Mon Sep 17 00:00:00 2001
From: Kihwal Lee <kihwal@apache.org>
Date: Thu, 14 Apr 2016 14:25:11 -0500
Subject: [PATCH 2174/2748] HDFS-10292. Add block id when client got Unable to
 close file exception. Contributed by Brahma Reddy
 Battula.

(cherry picked from commit 2c155afe2736a5571bbb3bdfb2fe6f9709227229)

 Conflicts:
	hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java

Change-Id: I3c817bbc1404d70f66344b1641cfb874b3ab952a
---
 .../org/apache/hadoop/hdfs/DFSOutputStream.java    |    4 ++--
 1 file changed, 2 insertions(+), 2 deletions(-)

diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java
index 234edb5..5e38a83 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java
@@ -2700,8 +2700,8 @@ private void completeFile(ExtendedBlock last) throws IOException {
         }
         try {
           if (retries == 0) {
-            throw new IOException("Unable to close file because the last block"
-                + " does not have enough number of replicas.");
+            throw new IOException("Unable to close file because the last block "
+                + last + " does not have enough number of replicas.");
           }
           retries--;
           Thread.sleep(localTimeout);
-- 
1.7.9.5

