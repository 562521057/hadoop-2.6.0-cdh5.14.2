From 36737b9c51b6f268de9dc026377fb3b3cb8eefb3 Mon Sep 17 00:00:00 2001
From: Vinayakumar B <vinayakumarb@apache.org>
Date: Wed, 3 Feb 2016 07:35:01 +0530
Subject: [PATCH 1171/2748] HDFS-9715. Check storage ID uniqueness on datanode
 startup (Contributed by Lei (Eddy) Xu)

(cherry picked from commit 04375756a5ed6e907ee7548469c2c508aebbafb7)
(cherry picked from commit 8577c0b6a2bc7e4966866eac2bd1f2a112d0a7c1)

Change-Id: I651a5a7cfa51f3c08a8149f5884e1d8775103b8c
---
 .../datanode/fsdataset/impl/FsDatasetImpl.java     |   46 ++++++++++++--------
 .../org/apache/hadoop/hdfs/UpgradeUtilities.java   |   10 +++++
 .../datanode/fsdataset/impl/TestFsDatasetImpl.java |   27 ++++++++++++
 .../hdfs/server/namenode/FSImageTestUtil.java      |   33 ++++++++++++--
 4 files changed, 93 insertions(+), 23 deletions(-)

diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java
index 06a7867..51cc52b 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java
@@ -351,6 +351,31 @@ public LengthInputStream getMetaDataInputStream(ExtendedBlock b)
     return volumeFailureInfos;
   }
 
+  /**
+   * Activate a volume to serve requests.
+   * @throws IOException if the storage UUID already exists.
+   */
+  private synchronized void activateVolume(
+      ReplicaMap replicaMap,
+      Storage.StorageDirectory sd, StorageType storageType,
+      FsVolumeReference ref) throws IOException {
+    DatanodeStorage dnStorage = storageMap.get(sd.getStorageUuid());
+    if (dnStorage != null) {
+      final String errorMsg = String.format(
+          "Found duplicated storage UUID: %s in %s.",
+          sd.getStorageUuid(), sd.getVersionFile());
+      LOG.error(errorMsg);
+      throw new IOException(errorMsg);
+    }
+    volumeMap.addAll(replicaMap);
+    storageMap.put(sd.getStorageUuid(),
+        new DatanodeStorage(sd.getStorageUuid(),
+            DatanodeStorage.State.NORMAL,
+            storageType));
+    asyncDiskService.addVolume(sd.getCurrentDir());
+    volumes.addVolume(ref);
+  }
+
   private void addVolume(Collection<StorageLocation> dataLocations,
       Storage.StorageDirectory sd) throws IOException {
     final File dir = sd.getCurrentDir();
@@ -366,16 +391,7 @@ private void addVolume(Collection<StorageLocation> dataLocations,
     ReplicaMap tempVolumeMap = new ReplicaMap(this);
     fsVolume.getVolumeMap(tempVolumeMap, ramDiskReplicaTracker);
 
-    synchronized (this) {
-      volumeMap.addAll(tempVolumeMap);
-      storageMap.put(sd.getStorageUuid(),
-          new DatanodeStorage(sd.getStorageUuid(),
-              DatanodeStorage.State.NORMAL,
-              storageType));
-      asyncDiskService.addVolume(sd.getCurrentDir());
-      volumes.addVolume(ref);
-    }
-
+    activateVolume(tempVolumeMap, sd, storageType, ref);
     LOG.info("Added volume - " + dir + ", StorageType: " + storageType);
   }
 
@@ -433,15 +449,7 @@ public void addVolume(final StorageLocation location,
     setupAsyncLazyPersistThread(fsVolume);
 
     builder.build();
-    synchronized (this) {
-      volumeMap.addAll(tempVolumeMap);
-      storageMap.put(sd.getStorageUuid(),
-          new DatanodeStorage(sd.getStorageUuid(),
-              DatanodeStorage.State.NORMAL,
-              storageType));
-      asyncDiskService.addVolume(sd.getCurrentDir());
-      volumes.addVolume(ref);
-    }
+    activateVolume(tempVolumeMap, sd, storageType, ref);
     LOG.info("Added volume - " + dir + ", StorageType: " + storageType);
   }
 
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/UpgradeUtilities.java b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/UpgradeUtilities.java
index dac26a0..b457283 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/UpgradeUtilities.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/UpgradeUtilities.java
@@ -30,6 +30,7 @@
 import java.net.URI;
 import java.util.Arrays;
 import java.util.Collections;
+import java.util.Properties;
 import java.util.zip.CRC32;
 
 import org.apache.hadoop.conf.Configuration;
@@ -49,6 +50,7 @@
 import org.apache.hadoop.hdfs.server.datanode.DataNodeLayoutVersion;
 import org.apache.hadoop.hdfs.server.datanode.DataStorage;
 import org.apache.hadoop.hdfs.server.namenode.NNStorage;
+import org.apache.hadoop.hdfs.server.protocol.DatanodeStorage;
 import org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols;
 
 import com.google.common.base.Preconditions;
@@ -378,6 +380,14 @@ public static void checksumContentsHelper(NodeType nodeType, File dir,
       localFS.copyToLocalFile(new Path(datanodeStorage.toString(), "current"),
                               new Path(newDir.toString()),
                               false);
+      // Change the storage UUID to avoid conflicts when DN starts up.
+      StorageDirectory sd = new StorageDirectory(
+          new File(datanodeStorage.toString()));
+      sd.setStorageUuid(DatanodeStorage.generateUuid());
+      Properties properties = Storage.readPropertiesFile(sd.getVersionFile());
+      properties.setProperty("storageID", sd.getStorageUuid());
+      Storage.writeProperties(sd.getVersionFile(), properties);
+
       retVal[i] = newDir;
     }
     return retVal;
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/TestFsDatasetImpl.java b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/TestFsDatasetImpl.java
index 38cc7e1..6ab3cec 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/TestFsDatasetImpl.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/TestFsDatasetImpl.java
@@ -19,6 +19,7 @@
 
 import com.google.common.collect.Lists;
 
+import org.apache.commons.io.FileUtils;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FileSystemTestHelper;
 import org.apache.hadoop.hdfs.DFSConfigKeys;
@@ -198,6 +199,32 @@ public void testAddVolumes() throws IOException {
     assertEquals(actualVolumes, expectedVolumes);
   }
 
+  @Test
+  public void testAddVolumeWithSameStorageUuid() throws IOException {
+    HdfsConfiguration conf = new HdfsConfiguration();
+    MiniDFSCluster cluster = new MiniDFSCluster.Builder(conf)
+        .numDataNodes(1).build();
+    try {
+      cluster.waitActive();
+      assertTrue(cluster.getDataNodes().get(0).isConnectedToNN(
+          cluster.getNameNode().getServiceRpcAddress()));
+
+      MiniDFSCluster.DataNodeProperties dn = cluster.stopDataNode(0);
+      File vol0 = cluster.getStorageDir(0, 0);
+      File vol1 = cluster.getStorageDir(0, 1);
+      Storage.StorageDirectory sd0 = new Storage.StorageDirectory(vol0);
+      Storage.StorageDirectory sd1 = new Storage.StorageDirectory(vol1);
+      FileUtils.copyFile(sd0.getVersionFile(), sd1.getVersionFile());
+
+      cluster.restartDataNode(dn, true);
+      cluster.waitActive();
+      assertFalse(cluster.getDataNodes().get(0).isConnectedToNN(
+          cluster.getNameNode().getServiceRpcAddress()));
+    } finally {
+      cluster.shutdown();
+    }
+  }
+
   @Test(timeout = 30000)
   public void testRemoveVolumes() throws IOException {
     // Feed FsDataset with block metadata.
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/FSImageTestUtil.java b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/FSImageTestUtil.java
index daf5ecb..aac9241 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/FSImageTestUtil.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/FSImageTestUtil.java
@@ -34,6 +34,8 @@
 import java.util.Collections;
 import java.util.EnumMap;
 import java.util.HashMap;
+import java.util.HashSet;
+import java.util.Iterator;
 import java.util.List;
 import java.util.Map;
 import java.util.Map.Entry;
@@ -313,21 +315,24 @@ public static void assertParallelFilesAreIdentical(List<File> dirs,
         fileList.add(f);
       }
     }
-    
+
+    Set<String> ignoredProperties = new HashSet<>();
+    ignoredProperties.add("storageID");
     for (List<File> sameNameList : groupedByName.values()) {
       if (sameNameList.get(0).isDirectory()) {
         // recurse
         assertParallelFilesAreIdentical(sameNameList, ignoredFileNames);
       } else {
         if ("VERSION".equals(sameNameList.get(0).getName())) {
-          assertPropertiesFilesSame(sameNameList.toArray(new File[0]));
+          assertPropertiesFilesSame(sameNameList.toArray(new File[0]),
+              ignoredProperties);
         } else {
           assertFileContentsSame(sameNameList.toArray(new File[0]));
         }
       }
     }  
   }
-  
+
   /**
    * Assert that a set of properties files all contain the same data.
    * We cannot simply check the md5sums here, since Properties files
@@ -339,6 +344,20 @@ public static void assertParallelFilesAreIdentical(List<File> dirs,
    */
   public static void assertPropertiesFilesSame(File[] propFiles)
       throws IOException {
+    assertPropertiesFilesSame(propFiles, null);
+  }
+
+  /**
+   * Assert that a set of properties files all contain the same data.
+   *
+   * @param propFiles the files to compare.
+   * @param ignoredProperties the property names to be ignored during
+   *                          comparison.
+   * @throws IOException if the files cannot be opened or read
+   * @throws AssertionError if the files differ
+   */
+  public static void assertPropertiesFilesSame(
+      File[] propFiles, Set<String> ignoredProperties) throws IOException {
     Set<Map.Entry<Object, Object>> prevProps = null;
     
     for (File f : propFiles) {
@@ -355,7 +374,13 @@ public static void assertPropertiesFilesSame(File[] propFiles)
       } else {
         Set<Entry<Object,Object>> diff =
           Sets.symmetricDifference(prevProps, props.entrySet());
-        if (!diff.isEmpty()) {
+        Iterator<Entry<Object, Object>> it = diff.iterator();
+        while (it.hasNext()) {
+          Entry<Object, Object> entry = it.next();
+          if (ignoredProperties != null &&
+              ignoredProperties.contains(entry.getKey())) {
+            continue;
+          }
           fail("Properties file " + f + " differs from " + propFiles[0]);
         }
       }
-- 
1.7.9.5

