From 9317e0dc138a37b6a2dd3f4728d13059069d6e52 Mon Sep 17 00:00:00 2001
From: Anu Engineer <aengineer@apache.org>
Date: Mon, 13 Jun 2016 14:02:04 -0700
Subject: [PATCH 1627/2748] HDFS-10517. DiskBalancer: Support help command.
 Contributed by Anu Engineer. (cherry picked from
 commit 84e8c0e27926daf6387c67352ff0ed5b5973e026)

HDFS-10540. Diskbalancer: The CLI error message for disk balancer is not enabled is not clear. Contributed by Anu Engineer.
(cherry picked from commit 701ed280a15a3e0d8d970756c8ae2098e72f33d6)

HDFS-10545. DiskBalancer: PlanCommand should use -fs instead of -uri to be consistent with other hdfs commands. Contributed by Anu Engineer.
(cherry picked from commit 13599a8a48ac5efa8c289342190c518e96112b78)

HDFS-10550. DiskBalancer: fix issue of order dependency in iteration in ReportCommand test. Contributed by Xiaobing Zhou.
(cherry picked from commit 90a0326537cedc800abe4fa3623ddc0606e7e5e1)

Conflicts:
	hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/diskbalancer/command/TestDiskBalancerCommand.java

HDFS-10557. Fix handling of the -fs Generic option. (Arpit Agarwal)
(cherry picked from commit 0eaa01c7c2f917d2c6fd13f05f0b8169cd92bcc2)

Change-Id: I01fd737d19632a7481d94e26e6f8d292f922a63e
---
 .../hadoop/hdfs/server/datanode/DiskBalancer.java  |    2 +
 .../server/diskbalancer/command/CancelCommand.java |   20 +-
 .../hdfs/server/diskbalancer/command/Command.java  |   33 +--
 .../diskbalancer/command/ExecuteCommand.java       |   17 +-
 .../server/diskbalancer/command/HelpCommand.java   |  108 ++++++++
 .../server/diskbalancer/command/PlanCommand.java   |   35 ++-
 .../server/diskbalancer/command/QueryCommand.java  |   15 +-
 .../server/diskbalancer/command/ReportCommand.java |   33 ++-
 .../server/diskbalancer/planner/GreedyPlanner.java |    8 +-
 .../org/apache/hadoop/hdfs/tools/DiskBalancer.java |  283 +++++++++++++-------
 .../src/site/markdown/HDFSDiskbalancer.md          |  120 +++++++++
 .../command/TestDiskBalancerCommand.java           |   86 +++---
 12 files changed, 550 insertions(+), 210 deletions(-)
 create mode 100644 hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/diskbalancer/command/HelpCommand.java
 create mode 100644 hadoop-hdfs-project/hadoop-hdfs/src/site/markdown/HDFSDiskbalancer.md

diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DiskBalancer.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DiskBalancer.java
index 7f768ea..ea09054 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DiskBalancer.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DiskBalancer.java
@@ -256,6 +256,8 @@ public String getVolumeNames() throws DiskBalancerException {
       }
       ObjectMapper mapper = new ObjectMapper();
       return mapper.writeValueAsString(pathMap);
+    } catch (DiskBalancerException ex) {
+      throw ex;
     } catch (IOException e) {
       throw new DiskBalancerException("Internal error, Unable to " +
           "create JSON string.", e,
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/diskbalancer/command/CancelCommand.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/diskbalancer/command/CancelCommand.java
index 3834d9b..740292d 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/diskbalancer/command/CancelCommand.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/diskbalancer/command/CancelCommand.java
@@ -21,6 +21,7 @@
 
 import com.google.common.base.Preconditions;
 import org.apache.commons.cli.CommandLine;
+import org.apache.commons.cli.HelpFormatter;
 import org.apache.commons.codec.digest.DigestUtils;
 import org.apache.commons.io.IOUtils;
 import org.apache.hadoop.conf.Configuration;
@@ -126,12 +127,21 @@ private void cancelPlanUsingHash(String nodeAddress, String hash) throws
 
   /**
    * Gets extended help for this command.
-   *
-   * @return Help Message
    */
   @Override
-  protected String getHelp() {
-    return "Cancels a running command. e.g -cancel <PlanFile> or -cancel " +
-        "<planID> -node <datanode>";
+  public void printHelp() {
+    String header = "Cancel command cancels a running disk balancer operation" +
+        ".\n\n";
+
+    String footer = "\nCancel command can be run via pointing to a plan file," +
+        " or by reading the plan ID using the query command and then using " +
+        "planID and hostname. Examples of how to run this command are \n" +
+        "hdfs diskbalancer -cancel <planfile> \n" +
+        "hdfs diskbalancer -cancel <planID> -node <hostname>";
+
+    HelpFormatter helpFormatter = new HelpFormatter();
+    helpFormatter.printHelp("hdfs diskbalancer -cancel <planFile> | -cancel " +
+        "<planID> -node <hostname>",
+        header, DiskBalancer.getCancelOptions(), footer);
   }
 }
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/diskbalancer/command/Command.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/diskbalancer/command/Command.java
index f43d730..3a4c541 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/diskbalancer/command/Command.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/diskbalancer/command/Command.java
@@ -45,9 +45,7 @@
 
 import java.io.IOException;
 import java.net.InetSocketAddress;
-import java.net.MalformedURLException;
 import java.net.URI;
-import java.net.URISyntaxException;
 import java.net.URL;
 import java.nio.charset.Charset;
 import java.nio.file.Files;
@@ -93,33 +91,14 @@ public Command(Configuration conf) {
    * Executes the Client Calls.
    *
    * @param cmd - CommandLine
-   * @throws IOException
-   * @throws URISyntaxException
+   * @throws Exception
    */
   public abstract void execute(CommandLine cmd) throws Exception;
 
   /**
    * Gets extended help for this command.
-   *
-   * @return Help Message
-   */
-  protected abstract String getHelp();
-
-  /**
-   * verifies user provided URL.
-   *
-   * @param uri - UrlString
-   * @return URL
-   * @throws URISyntaxException, MalformedURLException
    */
-  protected URI verifyURI(String uri)
-      throws URISyntaxException, MalformedURLException {
-    if ((uri == null) || uri.isEmpty()) {
-      throw new MalformedURLException(
-          "A valid URI is needed to execute this command.");
-    }
-    return new URI(uri);
-  }
+  public abstract void printHelp();
 
   /**
    * Process the URI and return the cluster with nodes setup. This is used in
@@ -132,11 +111,8 @@ protected URI verifyURI(String uri)
   protected DiskBalancerCluster readClusterInfo(CommandLine cmd) throws
       Exception {
     Preconditions.checkNotNull(cmd);
-    Preconditions
-        .checkState(cmd.getOptionValue(DiskBalancer.NAMENODEURI) != null,
-            "Required argument missing : uri");
 
-    setClusterURI(verifyURI(cmd.getOptionValue(DiskBalancer.NAMENODEURI)));
+    setClusterURI(FileSystem.getDefaultUri(getConf()));
     LOG.debug("using name node URI : {}", this.getClusterURI());
     ClusterConnector connector = ConnectorFactory.getCluster(this.clusterURI,
         getConf());
@@ -173,7 +149,7 @@ protected void setOutputPath(String path) throws IOException {
       diskBalancerLogs = new Path(path);
     }
     if (fs.exists(diskBalancerLogs)) {
-      LOG.error("Another Diskbalancer instance is running ? - Target " +
+      LOG.debug("Another Diskbalancer instance is running ? - Target " +
           "Directory already exists. {}", diskBalancerLogs);
       throw new IOException("Another DiskBalancer files already exist at the " +
           "target location. " + diskBalancerLogs.toString());
@@ -348,6 +324,7 @@ private static UserGroupInformation getUGI()
    *
    * @param fileName - fileName to open.
    * @return OutputStream.
+   * @throws IOException
    */
   protected FSDataOutputStream create(String fileName) throws IOException {
     Preconditions.checkNotNull(fileName);
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/diskbalancer/command/ExecuteCommand.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/diskbalancer/command/ExecuteCommand.java
index 85f2a86..5fd1f0a 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/diskbalancer/command/ExecuteCommand.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/diskbalancer/command/ExecuteCommand.java
@@ -21,6 +21,7 @@
 
 import com.google.common.base.Preconditions;
 import org.apache.commons.cli.CommandLine;
+import org.apache.commons.cli.HelpFormatter;
 import org.apache.commons.codec.digest.DigestUtils;
 import org.apache.commons.io.IOUtils;
 import org.apache.hadoop.conf.Configuration;
@@ -98,12 +99,18 @@ private void submitPlan(String planData) throws IOException {
 
   /**
    * Gets extended help for this command.
-   *
-   * @return Help Message
    */
   @Override
-  protected String getHelp() {
-    return "Execute command takes a plan and runs it against the node. e.g. " +
-        "hdfs diskbalancer -execute <nodename.plan.json> ";
+  public void printHelp() {
+    String header = "Execute command runs a submits a plan for execution on " +
+        "the given data node.\n\n";
+
+    String footer = "\nExecute command submits the job to data node and " +
+        "returns immediately. The state of job can be monitored via query " +
+        "command. ";
+
+    HelpFormatter helpFormatter = new HelpFormatter();
+    helpFormatter.printHelp("hdfs diskbalancer -execute <planfile>",
+        header, DiskBalancer.getExecuteOptions(), footer);
   }
 }
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/diskbalancer/command/HelpCommand.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/diskbalancer/command/HelpCommand.java
new file mode 100644
index 0000000..205df3d
--- /dev/null
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/diskbalancer/command/HelpCommand.java
@@ -0,0 +1,108 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ *
+ */
+
+package org.apache.hadoop.hdfs.server.diskbalancer.command;
+
+import com.google.common.base.Preconditions;
+import org.apache.commons.cli.CommandLine;
+import org.apache.commons.cli.HelpFormatter;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.hdfs.tools.DiskBalancer;
+
+/**
+ * Help Command prints out detailed help about each command.
+ */
+public class HelpCommand extends Command {
+
+  /**
+   * Constructs a help command.
+   *
+   * @param conf - config
+   */
+  public HelpCommand(Configuration conf) {
+    super(conf);
+  }
+
+  /**
+   * Executes the Client Calls.
+   *
+   * @param cmd - CommandLine
+   */
+  @Override
+  public void execute(CommandLine cmd) throws Exception {
+    LOG.debug("Processing help Command.");
+    if (cmd == null) {
+      this.printHelp();
+      return;
+    }
+
+    Preconditions.checkState(cmd.hasOption(DiskBalancer.HELP));
+    verifyCommandOptions(DiskBalancer.HELP, cmd);
+    String helpCommand = cmd.getOptionValue(DiskBalancer.HELP);
+    if (helpCommand == null || helpCommand.isEmpty()) {
+      this.printHelp();
+      return;
+    }
+
+    helpCommand = helpCommand.trim();
+    helpCommand = helpCommand.toLowerCase();
+    Command command = null;
+    switch (helpCommand) {
+    case DiskBalancer.PLAN:
+      command = new PlanCommand(getConf());
+      break;
+    case DiskBalancer.EXECUTE:
+      command = new ExecuteCommand(getConf());
+      break;
+    case DiskBalancer.QUERY:
+      command = new QueryCommand(getConf());
+      break;
+    case DiskBalancer.CANCEL:
+      command = new CancelCommand(getConf());
+      break;
+    case DiskBalancer.REPORT:
+      command = new ReportCommand(getConf(), null);
+      break;
+    default:
+      command = this;
+      break;
+    }
+    command.printHelp();
+
+  }
+
+  /**
+   * Gets extended help for this command.
+   */
+  @Override
+  public void printHelp() {
+    String header = "\nDiskBalancer distributes data evenly between " +
+        "different disks on a datanode. " +
+        "DiskBalancer operates by generating a plan, that tells datanode " +
+        "how to move data between disks. Users can execute a plan by " +
+        "submitting it to the datanode. \nTo get specific help on a " +
+        "particular command please run \n\n hdfs diskbalancer -help <command>.";
+
+    HelpFormatter helpFormatter = new HelpFormatter();
+    helpFormatter.printHelp("hdfs diskbalancer [command] [options]",
+        header, DiskBalancer.getHelpOptions(), "");
+  }
+
+
+}
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/diskbalancer/command/PlanCommand.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/diskbalancer/command/PlanCommand.java
index 4e2febb..20b4c6f 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/diskbalancer/command/PlanCommand.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/diskbalancer/command/PlanCommand.java
@@ -19,15 +19,18 @@
 
 import com.google.common.base.Preconditions;
 import org.apache.commons.cli.CommandLine;
+import org.apache.commons.cli.HelpFormatter;
 import org.apache.commons.lang.StringUtils;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FSDataOutputStream;
 import org.apache.hadoop.hdfs.DFSConfigKeys;
 import org.apache.hadoop.hdfs.protocol.ClientDatanodeProtocol;
 import org.apache.hadoop.hdfs.server.diskbalancer.DiskBalancerConstants;
-import org.apache.hadoop.hdfs.server.diskbalancer.datamodel.DiskBalancerDataNode;
+import org.apache.hadoop.hdfs.server.diskbalancer.datamodel
+    .DiskBalancerDataNode;
 import org.apache.hadoop.hdfs.server.diskbalancer.datamodel.DiskBalancerVolume;
-import org.apache.hadoop.hdfs.server.diskbalancer.datamodel.DiskBalancerVolumeSet;
+import org.apache.hadoop.hdfs.server.diskbalancer.datamodel
+    .DiskBalancerVolumeSet;
 import org.apache.hadoop.hdfs.server.diskbalancer.planner.NodePlan;
 import org.apache.hadoop.hdfs.server.diskbalancer.planner.Step;
 import org.apache.hadoop.hdfs.tools.DiskBalancer;
@@ -60,10 +63,9 @@ public PlanCommand(Configuration conf) {
     this.thresholdPercentage = 1;
     this.bandwidth = 0;
     this.maxError = 0;
-    addValidCommandParameters(DiskBalancer.NAMENODEURI, "Name Node URI or " +
-        "file URI for cluster");
-
-    addValidCommandParameters(DiskBalancer.OUTFILE, "Output file");
+    addValidCommandParameters(DiskBalancer.OUTFILE, "Output directory in " +
+        "HDFS. The generated plan will be written to a file in this " +
+        "directory.");
     addValidCommandParameters(DiskBalancer.BANDWIDTH, "Maximum Bandwidth to " +
         "be used while copying.");
     addValidCommandParameters(DiskBalancer.THRESHOLD, "Percentage skew that " +
@@ -182,12 +184,19 @@ private void populatePathNames(DiskBalancerDataNode node) throws IOException {
 
   /**
    * Gets extended help for this command.
-   *
-   * @return Help Message
    */
   @Override
-  protected String getHelp() {
-    return "This commands creates a disk balancer plan for given datanode";
+  public void printHelp() {
+    String header = "Creates a plan that describes how much data should be " +
+        "moved between disks.\n\n";
+
+    String footer = "\nPlan command creates a set of steps that represent a " +
+        "planned data move. A plan file can be executed on a data node, which" +
+        " will balance the data.";
+
+    HelpFormatter helpFormatter = new HelpFormatter();
+    helpFormatter.printHelp("hdfs diskbalancer -plan " +
+        "<hostname> [options]", header, DiskBalancer.getPlanOptions(), footer);
   }
 
   /**
@@ -221,9 +230,9 @@ static private void printToScreen(List<NodePlan> plans) {
 
     System.out.println(
         StringUtils.center("Source Disk", 30) +
-        StringUtils.center("Dest.Disk", 30) +
-        StringUtils.center("Size", 10) +
-        StringUtils.center("Type", 10));
+            StringUtils.center("Dest.Disk", 30) +
+            StringUtils.center("Size", 10) +
+            StringUtils.center("Type", 10));
 
     for (NodePlan plan : plans) {
       for (Step step : plan.getVolumeSetPlans()) {
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/diskbalancer/command/QueryCommand.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/diskbalancer/command/QueryCommand.java
index ea7dbcc..3a3b97f 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/diskbalancer/command/QueryCommand.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/diskbalancer/command/QueryCommand.java
@@ -21,6 +21,7 @@
 
 import com.google.common.base.Preconditions;
 import org.apache.commons.cli.CommandLine;
+import org.apache.commons.cli.HelpFormatter;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hdfs.DFSConfigKeys;
 import org.apache.hadoop.hdfs.protocol.ClientDatanodeProtocol;
@@ -86,11 +87,17 @@ public void execute(CommandLine cmd) throws Exception {
 
   /**
    * Gets extended help for this command.
-   *
-   * @return Help Message
    */
   @Override
-  protected String getHelp() {
-    return "Gets the status of disk balancing on a given node";
+  public void printHelp() {
+    String header = "Query Plan queries a given data node about the " +
+        "current state of disk balancer execution.\n\n";
+
+    String footer = "\nQuery command retrievs the plan ID and the current " +
+        "running state. ";
+
+    HelpFormatter helpFormatter = new HelpFormatter();
+    helpFormatter.printHelp("hdfs diskbalancer -query <hostname>  [options]",
+        header, DiskBalancer.getQueryOptions(), footer);
   }
 }
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/diskbalancer/command/ReportCommand.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/diskbalancer/command/ReportCommand.java
index acf9ff2..9f8e399 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/diskbalancer/command/ReportCommand.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/diskbalancer/command/ReportCommand.java
@@ -19,9 +19,11 @@
 
 import java.io.PrintStream;
 import java.util.Collections;
+import java.util.List;
 import java.util.ListIterator;
 
 import org.apache.commons.cli.CommandLine;
+import org.apache.commons.cli.HelpFormatter;
 import org.apache.commons.lang.StringUtils;
 import org.apache.commons.lang.text.StrBuilder;
 import org.apache.hadoop.conf.Configuration;
@@ -31,6 +33,7 @@
 import org.apache.hadoop.hdfs.tools.DiskBalancer;
 
 import com.google.common.base.Preconditions;
+import com.google.common.collect.Lists;
 
 /**
  * Executes the report command.
@@ -163,9 +166,10 @@ private void handleNodeReport(final CommandLine cmd, StrBuilder result,
             dbdn.getVolumeCount(),
             dbdn.getNodeDataDensity()));
 
+        List<String> volumeList = Lists.newArrayList();
         for (DiskBalancerVolumeSet vset : dbdn.getVolumeSets().values()) {
           for (DiskBalancerVolume vol : vset.getVolumes()) {
-            result.appendln(String.format(volumeFormat,
+            volumeList.add(String.format(volumeFormat,
                 vol.getStorageType(),
                 vol.getPath(),
                 vol.getUsedRatio(),
@@ -180,18 +184,31 @@ private void handleNodeReport(final CommandLine cmd, StrBuilder result,
                 vol.isTransient() ? trueStr : falseStr));
           }
         }
+
+        Collections.sort(volumeList);
+        result.appendln(
+            StringUtils.join(volumeList.toArray(), System.lineSeparator()));
       }
     }
   }
 
+  /**
+   * Prints the help message.
+   */
   @Override
-  protected String getHelp() {
-    return "Report volume information for a specific DataNode or top X "
-        + "one(s) benefiting from running DiskBalancer, "
-        + "top defaults to " + getDefaultTop() + ". E.g.:\n"
-        + "hdfs diskbalancer -uri http://namenode.uri -report\n"
-        + "hdfs diskbalancer -uri http://namenode.uri -report -top 5\n"
-        + "hdfs diskbalancer -uri http://namenode.uri -report "
+  public void printHelp() {
+    String header = "Report command reports the volume information of a given" +
+        " datanode, or prints out the list of nodes that will benefit from " +
+        "running disk balancer. Top defaults to " + getDefaultTop();
+    String footer = ". E.g.:\n"
+        + "hdfs diskbalancer -report\n"
+        + "hdfs diskbalancer -report -top 5\n"
+        + "hdfs diskbalancer -report "
         + "-node {DataNodeID | IP | Hostname}";
+
+    HelpFormatter helpFormatter = new HelpFormatter();
+    helpFormatter.printHelp("hdfs diskbalancer -fs http://namenode.uri " +
+        "-report [options]",
+        header, DiskBalancer.getReportOptions(), footer);
   }
 }
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/diskbalancer/planner/GreedyPlanner.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/diskbalancer/planner/GreedyPlanner.java
index b3d51c4..0df9843 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/diskbalancer/planner/GreedyPlanner.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/diskbalancer/planner/GreedyPlanner.java
@@ -67,7 +67,8 @@ public NodePlan plan(DiskBalancerDataNode node) throws Exception {
     long startTime = Time.monotonicNow();
     NodePlan plan = new NodePlan(node.getDataNodeName(),
         node.getDataNodePort());
-    LOG.info("Starting plan for Node : " + node.getDataNodeUUID());
+    LOG.info("Starting plan for Node : {}:{}",
+        node.getDataNodeName(), node.getDataNodePort());
     while (node.isBalancingNeeded(this.threshold)) {
       for (DiskBalancerVolumeSet vSet : node.getVolumeSets().values()) {
         balanceVolumeSet(node, vSet, plan);
@@ -76,8 +77,9 @@ public NodePlan plan(DiskBalancerDataNode node) throws Exception {
 
     long endTime = Time.monotonicNow();
     String message = String
-        .format("Compute Plan for Node : %s took %d ms ",
-            node.getDataNodeUUID(), endTime - startTime);
+        .format("Compute Plan for Node : %s:%d took %d ms ",
+            node.getDataNodeName(), node.getDataNodePort(),
+            endTime - startTime);
     LOG.info(message);
     return plan;
   }
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/DiskBalancer.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/DiskBalancer.java
index 1251e96..612aa2c 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/DiskBalancer.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/DiskBalancer.java
@@ -18,8 +18,8 @@
 
 import org.apache.commons.cli.BasicParser;
 import org.apache.commons.cli.CommandLine;
-import org.apache.commons.cli.HelpFormatter;
 import org.apache.commons.cli.Option;
+import org.apache.commons.cli.OptionBuilder;
 import org.apache.commons.cli.Options;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.conf.Configured;
@@ -27,6 +27,7 @@
 import org.apache.hadoop.hdfs.server.diskbalancer.command.CancelCommand;
 import org.apache.hadoop.hdfs.server.diskbalancer.command.Command;
 import org.apache.hadoop.hdfs.server.diskbalancer.command.ExecuteCommand;
+import org.apache.hadoop.hdfs.server.diskbalancer.command.HelpCommand;
 import org.apache.hadoop.hdfs.server.diskbalancer.command.PlanCommand;
 import org.apache.hadoop.hdfs.server.diskbalancer.command.QueryCommand;
 import org.apache.hadoop.hdfs.server.diskbalancer.command.ReportCommand;
@@ -35,9 +36,7 @@
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
-import java.io.IOException;
 import java.io.PrintStream;
-import java.net.URISyntaxException;
 
 /**
  * DiskBalancer is a tool that can be used to ensure that data is spread evenly
@@ -53,16 +52,6 @@
  */
 public class DiskBalancer extends Configured implements Tool {
   /**
-   * NameNodeURI can point to either a real namenode, or a json file that
-   * contains the diskBalancer data in json form, that jsonNodeConnector knows
-   * how to deserialize.
-   * <p>
-   * Expected formats are :
-   * <p>
-   * hdfs://namenode.uri or file:///data/myCluster.json
-   */
-  public static final String NAMENODEURI = "uri";
-  /**
    * Computes a plan for a given set of nodes.
    */
   public static final String PLAN = "plan";
@@ -139,6 +128,13 @@
   private static final Logger LOG =
       LoggerFactory.getLogger(DiskBalancer.class);
 
+  private static final Options PLAN_OPTIONS = new Options();
+  private static final Options EXECUTE_OPTIONS = new Options();
+  private static final Options QUERY_OPTIONS = new Options();
+  private static final Options HELP_OPTIONS = new Options();
+  private static final Options CANCEL_OPTIONS = new Options();
+  private static final Options REPORT_OPTIONS = new Options();
+
   /**
    * Construct a DiskBalancer.
    *
@@ -161,7 +157,7 @@ public static void main(String[] argv) throws Exception {
       res = ToolRunner.run(shell, argv);
     } catch (Exception ex) {
       LOG.error(ex.toString());
-      System.exit(1);
+      res = 1;
     }
     System.exit(res);
   }
@@ -182,7 +178,7 @@ public int run(String[] args) throws Exception {
    * Execute the command with the given arguments.
    *
    * @param args command specific arguments.
-   * @param out the output stream used for printing
+   * @param out  the output stream used for printing
    * @return exit code.
    * @throws Exception
    */
@@ -200,6 +196,7 @@ public int run(String[] args, final PrintStream out) throws Exception {
   private Options getOpts() {
     Options opts = new Options();
     addPlanCommands(opts);
+    addHelpCommands(opts);
     addExecuteCommands(opts);
     addQueryCommands(opts);
     addCancelCommands(opts);
@@ -208,101 +205,208 @@ private Options getOpts() {
   }
 
   /**
+   * Returns Plan options.
+   *
+   * @return Options.
+   */
+  public static Options getPlanOptions() {
+    return PLAN_OPTIONS;
+  }
+
+  /**
+   * Returns help options.
+   *
+   * @return - help options.
+   */
+  public static Options getHelpOptions() {
+    return HELP_OPTIONS;
+  }
+
+  /**
+   * Retuns execute options.
+   *
+   * @return - execute options.
+   */
+  public static Options getExecuteOptions() {
+    return EXECUTE_OPTIONS;
+  }
+
+  /**
+   * Returns Query Options.
+   *
+   * @return query Options
+   */
+  public static Options getQueryOptions() {
+    return QUERY_OPTIONS;
+  }
+
+  /**
+   * Returns Cancel Options.
+   *
+   * @return Options
+   */
+  public static Options getCancelOptions() {
+    return CANCEL_OPTIONS;
+  }
+
+  /**
+   * Returns Report Options.
+   *
+   * @return Options
+   */
+  public static Options getReportOptions() {
+    return REPORT_OPTIONS;
+  }
+
+  /**
    * Adds commands for plan command.
    *
-   * @param opt - Options
+   * @return Options.
    */
   private void addPlanCommands(Options opt) {
 
-    Option nameNodeUri =
-        new Option(NAMENODEURI, true, "NameNode URI. e.g http://namenode" +
-            ".mycluster.com or file:///myCluster" +
-            ".json");
-    opt.addOption(nameNodeUri);
+    Option plan = OptionBuilder.withLongOpt(PLAN)
+        .withDescription("creates a plan for datanode.")
+        .hasArg()
+        .create();
+    getPlanOptions().addOption(plan);
+    opt.addOption(plan);
 
-    Option outFile =
-        new Option(OUTFILE, true, "File to write output to, if not specified " +
-            "defaults will be used." +
-            "e.g -out outfile.txt");
-    opt.addOption(outFile);
 
-    Option plan = new Option(PLAN, true , "create a plan for the given node. " +
-        "e.g -plan <nodename> | <nodeIP> | <nodeUUID>");
-    opt.addOption(plan);
+    Option outFile = OptionBuilder.withLongOpt(OUTFILE)
+        .hasArg()
+        .withDescription("File to write output to, if not specified " +
+            "defaults will be used.")
+        .create();
+    getPlanOptions().addOption(outFile);
+    opt.addOption(outFile);
 
-    Option bandwidth = new Option(BANDWIDTH, true, "Maximum disk bandwidth to" +
-        " be consumed by diskBalancer. " +
-        "Expressed as MBs per second.");
+    Option bandwidth = OptionBuilder.withLongOpt(BANDWIDTH)
+        .hasArg()
+        .withDescription("Maximum disk bandwidth to be consumed by " +
+            "diskBalancer. e.g. 10")
+        .create();
+    getPlanOptions().addOption(bandwidth);
     opt.addOption(bandwidth);
 
-    Option threshold = new Option(THRESHOLD, true, "Percentage skew that we " +
-        "tolerate before diskbalancer starts working or stops when reaching " +
-        "that range.");
+    Option threshold = OptionBuilder.withLongOpt(THRESHOLD)
+        .hasArg()
+        .withDescription("Percentage skew that we" +
+            "tolerate before diskbalancer starts working e.g. 10")
+        .create();
+    getPlanOptions().addOption(threshold);
     opt.addOption(threshold);
 
-    Option maxErrors = new Option(MAXERROR, true, "Describes how many errors " +
-        "can be tolerated while copying between a pair of disks.");
-    opt.addOption(maxErrors);
 
-    Option help =
-        new Option(HELP, true, "Help about a command or this message");
-    opt.addOption(help);
+    Option maxError = OptionBuilder.withLongOpt(MAXERROR)
+        .hasArg()
+        .withDescription("Describes how many errors " +
+            "can be tolerated while copying between a pair of disks.")
+        .create();
+    getPlanOptions().addOption(maxError);
+    opt.addOption(maxError);
 
-    Option verbose = new Option(VERBOSE, "Print out the summary of the plan");
+    Option verbose = OptionBuilder.withLongOpt(VERBOSE)
+        .withDescription("Print out the summary of the plan on console")
+        .create();
+    getPlanOptions().addOption(verbose);
     opt.addOption(verbose);
+  }
 
+  /**
+   * Adds Help to the options.
+   */
+  private void addHelpCommands(Options opt) {
+    Option help = OptionBuilder.withLongOpt(HELP)
+        .hasOptionalArg()
+        .withDescription("valid commands are plan | execute | query | cancel" +
+            " | report")
+        .create();
+    getHelpOptions().addOption(help);
+    opt.addOption(help);
   }
 
   /**
    * Adds execute command options.
+   *
    * @param opt Options
    */
   private void addExecuteCommands(Options opt) {
-    Option execute = new Option(EXECUTE, true , "Takes a plan file and " +
-        "submits it for execution to the datanode. e.g -execute <planfile>");
+    Option execute = OptionBuilder.withLongOpt(EXECUTE)
+        .hasArg()
+        .withDescription("Takes a plan file and " +
+            "submits it for execution by the datanode.")
+        .create();
+    getExecuteOptions().addOption(execute);
     opt.addOption(execute);
   }
 
   /**
    * Adds query command options.
+   *
    * @param opt Options
    */
   private void addQueryCommands(Options opt) {
-    Option query = new Option(QUERY, true, "Queries the disk balancer " +
-        "status of a given datanode. e.g. -query <nodename>");
+    Option query = OptionBuilder.withLongOpt(QUERY)
+        .hasArg()
+        .withDescription("Queries the disk balancer " +
+            "status of a given datanode.")
+        .create();
+    getQueryOptions().addOption(query);
     opt.addOption(query);
+
+    // Please note: Adding this only to Query options since -v is already
+    // added to global table.
+    Option verbose = OptionBuilder.withLongOpt(VERBOSE)
+        .withDescription("Prints details of the plan that is being executed " +
+            "on the node.")
+        .create();
+    getQueryOptions().addOption(verbose);
   }
 
   /**
    * Adds cancel command options.
+   *
    * @param opt Options
    */
   private void addCancelCommands(Options opt) {
-    Option cancel = new Option(CANCEL, true, "Cancels a running plan. -cancel" +
-        " <planFile> or -cancel <planID> -node <datanode:port>");
+    Option cancel = OptionBuilder.withLongOpt(CANCEL)
+        .hasArg()
+        .withDescription("Cancels a running plan using a plan file.")
+        .create();
+    getCancelOptions().addOption(cancel);
     opt.addOption(cancel);
-    Option node = new Option(NODE, true, "Name of the datanode in name:port " +
-        "format");
+
+    Option node = OptionBuilder.withLongOpt(NODE)
+        .hasArg()
+        .withDescription("Cancels a running plan using a plan ID and hostName")
+        .create();
+
+    getCancelOptions().addOption(node);
     opt.addOption(node);
   }
 
   /**
    * Adds report command options.
+   *
    * @param opt Options
    */
   private void addReportCommands(Options opt) {
-    Option report = new Option(REPORT, false,
-        "Report volume information of DataNode(s)"
-            + " benefiting from running DiskBalancer. "
-            + "-report [top -X] | [-node {DataNodeID | IP | Hostname}].");
+    Option report = OptionBuilder.withLongOpt(REPORT)
+        .withDescription("List nodes that will benefit from running " +
+            "DiskBalancer.")
+        .create();
+    getReportOptions().addOption(report);
     opt.addOption(report);
 
     Option top = new Option(TOP, true,
-        "specify the top number of nodes to be processed.");
+        "specify the number of nodes to be listed which has data imbalance.");
+    getReportOptions().addOption(top);
     opt.addOption(top);
 
     Option node = new Option(NODE, true,
-        "Name of the datanode in the format of DataNodeID, IP or hostname.");
+        "Datanode address, it can be DataNodeID, IP or hostname.");
+    getReportOptions().addOption(node);
     opt.addOption(node);
   }
 
@@ -322,51 +426,44 @@ private CommandLine parseArgs(String[] argv, Options opts)
   /**
    * Dispatches calls to the right command Handler classes.
    *
-   * @param cmd - CommandLine
+   * @param cmd  - CommandLine
    * @param opts options of command line
-   * @param out the output stream used for printing
-   * @throws IOException
-   * @throws URISyntaxException
+   * @param out  the output stream used for printing
    */
   private int dispatch(CommandLine cmd, Options opts, final PrintStream out)
-      throws IOException, URISyntaxException {
+      throws Exception {
     Command currentCommand = null;
+    if (cmd.hasOption(DiskBalancer.PLAN)) {
+      currentCommand = new PlanCommand(getConf());
+    }
 
-    try {
+    if (cmd.hasOption(DiskBalancer.EXECUTE)) {
+      currentCommand = new ExecuteCommand(getConf());
+    }
 
-      if (cmd.hasOption(DiskBalancer.PLAN)) {
-        currentCommand = new PlanCommand(getConf());
-      }
-
-      if(cmd.hasOption(DiskBalancer.EXECUTE)) {
-        currentCommand = new ExecuteCommand(getConf());
-      }
-
-      if(cmd.hasOption(DiskBalancer.QUERY)) {
-        currentCommand = new QueryCommand(getConf());
-      }
-
-      if(cmd.hasOption(DiskBalancer.CANCEL)) {
-        currentCommand = new CancelCommand(getConf());
-      }
-
-      if (cmd.hasOption(DiskBalancer.REPORT)) {
-        currentCommand = new ReportCommand(getConf(), out);
-      }
-
-      if(currentCommand == null) {
-        HelpFormatter helpFormatter = new HelpFormatter();
-        helpFormatter.printHelp(80, "hdfs diskbalancer -uri [args]",
-            "disk balancer commands", opts,
-            "Please correct your command and try again.");
-        return 1;
-      }
-      currentCommand.execute(cmd);
-    } catch (Exception ex) {
-      System.err.printf(ex.getMessage());
+    if (cmd.hasOption(DiskBalancer.QUERY)) {
+      currentCommand = new QueryCommand(getConf());
+    }
+
+    if (cmd.hasOption(DiskBalancer.CANCEL)) {
+      currentCommand = new CancelCommand(getConf());
+    }
+
+    if (cmd.hasOption(DiskBalancer.REPORT)) {
+      currentCommand = new ReportCommand(getConf(), out);
+    }
+
+    if (cmd.hasOption(DiskBalancer.HELP)) {
+      currentCommand = new HelpCommand(getConf());
+    }
+
+    // Invoke main help here.
+    if (currentCommand == null) {
+      new HelpCommand(getConf()).execute(null);
       return 1;
     }
+
+    currentCommand.execute(cmd);
     return 0;
   }
-
 }
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/site/markdown/HDFSDiskbalancer.md b/hadoop-hdfs-project/hadoop-hdfs/src/site/markdown/HDFSDiskbalancer.md
new file mode 100644
index 0000000..c5f73cd
--- /dev/null
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/site/markdown/HDFSDiskbalancer.md
@@ -0,0 +1,120 @@
+<!---
+  Licensed under the Apache License, Version 2.0 (the "License");
+  you may not use this file except in compliance with the License.
+  You may obtain a copy of the License at
+
+   http://www.apache.org/licenses/LICENSE-2.0
+
+  Unless required by applicable law or agreed to in writing, software
+  distributed under the License is distributed on an "AS IS" BASIS,
+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+  See the License for the specific language governing permissions and
+  limitations under the License. See accompanying LICENSE file.
+-->
+
+HDFS Disk Balancer
+===================
+
+* [Overview](#Overview)
+* [Architecture](#Architecture)
+* [Commands](#Commands)
+* [Settings](#Settings)
+
+
+Overview
+--------
+
+Diskbalancer is a command line tool that distributes data evenly on all disks of a datanode.
+This tool is different from  [Balancer](./HdfsUserGuide.html#Balancer)  which
+takes care of cluster-wide data balancing. Data can have uneven spread between
+disks on a node due to several reasons. This can happen due to large amount of
+writes and deletes or due to a disk replacement.This tool operates against a given datanode and moves blocks from one disk to another.
+
+
+
+Architecture
+------------
+
+Disk Balancer operates by creating a plan and goes on to execute that plan on the datanode.
+A plan is a set of statements that describe how much data should move between two disks.
+A plan is composed of multiple move steps. A move step has source disk, destination
+disk and number of bytes to move.A plan can be executed against an operational data node. Disk balancer should not
+interfere with other processes since it throttles how much data is copied
+every second. Please note that disk balancer is not enabled by default on a cluster.
+To enable diskbalancer `dfs.disk.balancer.enabled` must be set to `true` in hdfs-site.xml.
+
+
+Commands
+--------
+The following sections discusses what commands are supported by disk balancer
+ and how to use them.
+
+### Plan
+
+ The plan command can be run against a given datanode by running
+
+ `hdfs diskbalancer -plan node1.mycluster.com`
+
+ The command accepts [Generic Options](../hadoop-common/CommandsManual.html#Generic_Options).
+
+ The plan command also has a set of parameters that allows user to control
+ the output and execution of the plan.
+
+| COMMAND\_OPTION    | Description |
+|:---- |:---- |
+| `-out`| Allows user to control the output location of the plan file.|
+| `-bandwidth`| Since datanode is operational and might be running other jobs, diskbalancer limits the amount of data moved per second. This parameter allows user to set the maximum bandwidth to be used. This is not required to be set since diskBalancer will use the deafult bandwidth if this is not specified.|
+| `-thresholdPercentage`| Since we operate against a snap-shot of datanode, themove operations have a tolerance percentage to declare success. If user specifies 10% and move operation is say 20GB in size, if we can move 18GB that operation is considered successful. This is to accomodate the changes in datanode in real time. This parameter is not needed and a default is used if not specified.|
+| `-maxerror` | Max error allows users to specify how many block copy operations must fail before we abort a move step. Once again, this is not a needed parameter and a system-default is used if not specified.|
+| `-v`| Verbose mode, specifying this parameter forces the plan command to print out a summary of the plan on stdout.|
+
+The plan command writes two output files. They are `<nodename>.before.json` which
+captures the state of the datanode before the diskbalancer is run, and `<nodename>.plan.json`.
+
+### Execute
+
+Execute command takes a plan command executes it against the datanode that plan was generated against.
+
+`hdfs diskbalancer -execute /system/diskbalancer/nodename.plan.json`
+
+This executes the plan by reading datanode’s address from the plan file.
+
+### Query
+
+Query command gets the current status of the diskbalancer from a datanode.
+
+`hdfs diskbalancer -query nodename.mycluster.com`
+
+| COMMAND\_OPTION | Description |
+|:---- |:---- |
+|`-v` | Verbose mode, Prints out status of individual moves|
+
+
+### Cancel
+Cancel command cancels a running plan. Restarting datanode has the same effect as cancel command since plan information on the datanode is transient.
+
+`hdfs diskbalancer -cancel /system/diskbalancer/nodename.plan.json`
+
+or
+
+`hdfs diskbalancer -cancel planID -node nodename`
+
+Plan ID can be read from datanode using query command.
+
+### Report
+Report command provides detailed report about a node.
+
+`hdfs diskbalancer -fs http://namenode.uri -report -node {DataNodeID | IP | Hostname}`
+
+
+Settings
+--------
+
+There is a set of diskbalancer settings that can be controlled via hdfs-site.xml
+
+| Setting | Description |
+|:---- |:---- |
+|`dfs.disk.balancer.enabled`| This parameter controls if diskbalancer is enabled for a cluster. if this is not enabled, any execute command will be rejected by the datanode.The default value is false.|
+|`dfs.disk.balancer.max.disk.throughputInMBperSec` | This controls the maximum disk bandwidth consumed by diskbalancer while copying data. If a value like 10MB is specified then diskbalancer on the average will only copy 10MB/S. The default value is 10MB/S.|
+|`dfs.disk.balancer.max.disk.errors`| sets the value of maximum number of errors we can ignore for a specific move between two disks before it is abandoned. For example, if a plan has 3 pair of disks to copy between , and the first disk set encounters more than 5 errors, then we abandon the first copy and start the second copy in the plan. The default value of max errors is set to 5.|
+|`dfs.disk.balancer.block.tolerance.percent`| The tolerance percent sepcifies when we have reached a good enough value for any copy step. For example, if you specify 10% then getting close to 10% of the target value is good enough.|
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/diskbalancer/command/TestDiskBalancerCommand.java b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/diskbalancer/command/TestDiskBalancerCommand.java
index dbc3837..c1c137d 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/diskbalancer/command/TestDiskBalancerCommand.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/diskbalancer/command/TestDiskBalancerCommand.java
@@ -31,6 +31,7 @@
 
 import org.apache.commons.lang.StringUtils;
 import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.hdfs.DFSConfigKeys;
 import org.apache.hadoop.hdfs.HdfsConfiguration;
 import org.apache.hadoop.hdfs.MiniDFSCluster;
@@ -71,9 +72,10 @@ public void tearDown() throws Exception {
     }
   }
 
-  private void testReportSimple() throws Exception {
-    final String cmdLine = String.format("hdfs diskbalancer -uri %s -report",
-        clusterJson.toString());
+  /* test basic report */
+  @Test(timeout=60000)
+  public void testReportSimple() throws Exception {
+    final String cmdLine = "hdfs diskbalancer -report";
     final List<String> outputs = runCommand(cmdLine);
 
     assertThat(
@@ -98,9 +100,10 @@ private void testReportSimple() throws Exception {
 
   }
 
-  private void testReportLessThanTotal() throws Exception {
-    final String cmdLine = String.format(
-        "hdfs diskbalancer -uri %s -report -top 32", clusterJson.toString());
+  /* test less than 64 DataNode(s) as total, e.g., -report -top 32 */
+  @Test(timeout=60000)
+  public void testReportLessThanTotal() throws Exception {
+    final String cmdLine = "hdfs diskbalancer -report -top 32";
     final List<String> outputs = runCommand(cmdLine);
 
     assertThat(
@@ -120,9 +123,10 @@ private void testReportLessThanTotal() throws Exception {
             containsString("9 volumes with node data density 1.97"))));
   }
 
-  private void testReportMoreThanTotal() throws Exception {
-    final String cmdLine = String.format(
-        "hdfs diskbalancer -uri %s -report -top 128", clusterJson.toString());
+  /* test more than 64 DataNode(s) as total, e.g., -report -top 128 */
+  @Test(timeout=60000)
+  public void testReportMoreThanTotal() throws Exception {
+    final String cmdLine = "hdfs diskbalancer -report -top 128";
     final List<String> outputs = runCommand(cmdLine);
 
     assertThat(
@@ -143,9 +147,10 @@ private void testReportMoreThanTotal() throws Exception {
 
   }
 
-  private void testReportInvalidTopLimit() throws Exception {
-    final String cmdLine = String.format(
-        "hdfs diskbalancer -uri %s -report -top xx", clusterJson.toString());
+  /* test invalid top limit, e.g., -report -top xx */
+  @Test(timeout=60000)
+  public void testReportInvalidTopLimit() throws Exception {
+    final String cmdLine = "hdfs diskbalancer -report -top xx";
     final List<String> outputs = runCommand(cmdLine);
 
     assertThat(
@@ -169,12 +174,11 @@ private void testReportInvalidTopLimit() throws Exception {
             containsString("9 volumes with node data density 1.97"))));
   }
 
-  private void testReportNode() throws Exception {
-    final String cmdLine = String
-        .format(
-            "hdfs diskbalancer -uri %s -report -node "
-                + "a87654a9-54c7-4693-8dd9-c9c7021dc340",
-            clusterJson.toString());
+  @Test(timeout=60000)
+  public void testReportNode() throws Exception {
+    final String cmdLine =
+            "hdfs diskbalancer -report -node " +
+            "a87654a9-54c7-4693-8dd9-c9c7021dc340";
     final List<String> outputs = runCommand(cmdLine);
 
     assertThat(
@@ -192,9 +196,9 @@ private void testReportNode() throws Exception {
     assertThat(
         outputs.get(3),
         is(allOf(containsString("DISK"),
-            containsString("/tmp/disk/xx3j3ph3zd"),
-            containsString("0.72 used: 289544224916/400000000000"),
-            containsString("0.28 free: 110455775084/400000000000"))));
+            containsString("/tmp/disk/KmHefYNURo"),
+            containsString("0.20 used: 39160240782/200000000000"),
+            containsString("0.80 free: 160839759218/200000000000"))));
     assertThat(
         outputs.get(4),
         is(allOf(containsString("DISK"),
@@ -204,16 +208,15 @@ private void testReportNode() throws Exception {
     assertThat(
         outputs.get(5),
         is(allOf(containsString("DISK"),
-            containsString("DISK"),
-            containsString("/tmp/disk/KmHefYNURo"),
-            containsString("0.20 used: 39160240782/200000000000"),
-            containsString("0.80 free: 160839759218/200000000000"))));
+            containsString("/tmp/disk/xx3j3ph3zd"),
+            containsString("0.72 used: 289544224916/400000000000"),
+            containsString("0.28 free: 110455775084/400000000000"))));
     assertThat(
         outputs.get(6),
         is(allOf(containsString("RAM_DISK"),
-            containsString("/tmp/disk/MXRyYsCz3U"),
-            containsString("0.55 used: 438102096853/800000000000"),
-            containsString("0.45 free: 361897903147/800000000000"))));
+            containsString("/tmp/disk/BoBlQFxhfw"),
+            containsString("0.60 used: 477590453390/800000000000"),
+            containsString("0.40 free: 322409546610/800000000000"))));
     assertThat(
         outputs.get(7),
         is(allOf(containsString("RAM_DISK"),
@@ -223,9 +226,9 @@ private void testReportNode() throws Exception {
     assertThat(
         outputs.get(8),
         is(allOf(containsString("RAM_DISK"),
-            containsString("/tmp/disk/BoBlQFxhfw"),
-            containsString("0.60 used: 477590453390/800000000000"),
-            containsString("0.40 free: 322409546610/800000000000"))));
+            containsString("/tmp/disk/MXRyYsCz3U"),
+            containsString("0.55 used: 438102096853/800000000000"),
+            containsString("0.45 free: 361897903147/800000000000"))));
     assertThat(
         outputs.get(9),
         is(allOf(containsString("SSD"),
@@ -247,26 +250,6 @@ private void testReportNode() throws Exception {
   }
 
   @Test(timeout=60000)
-  public void testReportCommmand() throws Exception {
-
-    /* test basic report */
-    testReportSimple();
-
-    /* test less than 64 DataNode(s) as total, e.g., -report -top 32 */
-    testReportLessThanTotal();
-
-    /* test more than 64 DataNode(s) as total, e.g., -report -top 128 */
-    testReportMoreThanTotal();
-
-    /* test invalid top limit, e.g., -report -top xx */
-    testReportInvalidTopLimit();
-
-    /* test -report -node DataNodeID */
-    // CLOUDERA-BUILD: temporarily disable this test.
-    // testReportNode();
-  }
-
-  @Test
   public void testReadClusterFromJson() throws Exception {
     Configuration conf = new HdfsConfiguration();
     conf.setBoolean(DFSConfigKeys.DFS_DISK_BALANCER_ENABLED, true);
@@ -286,6 +269,7 @@ public void testReadClusterFromJson() throws Exception {
     org.apache.hadoop.hdfs.tools.DiskBalancer db =
         new org.apache.hadoop.hdfs.tools.DiskBalancer(conf);
 
+    FileSystem.setDefaultUri(conf, clusterJson);
     ByteArrayOutputStream bufOut = new ByteArrayOutputStream();
     PrintStream out = new PrintStream(bufOut);
     db.run(cmds, out);
-- 
1.7.9.5

