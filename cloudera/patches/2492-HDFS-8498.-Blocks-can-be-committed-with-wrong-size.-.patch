From 68a419d7c0920f8e2b8a020b3b18a31979c7d074 Mon Sep 17 00:00:00 2001
From: Yongjun Zhang <yzhang@cloudera.com>
Date: Fri, 26 May 2017 22:07:57 -0700
Subject: [PATCH 2492/2748] HDFS-8498. Blocks can be committed with wrong
 size. Contributed by Jing Zhao. Backport
 HDFS-11732 by Zhe Zhang.

(cherry picked from commit 0823fb72a9afb696c357e0b611f2e04b367b8f24)

 Conflicts:
	hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java
	hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSOutputStream.java

Change-Id: Ib7d642868c9a36663d7f6bcfc3df0c2d39619349
---
 .../org/apache/hadoop/hdfs/DFSOutputStream.java    |  113 +++++++++++++-------
 1 file changed, 77 insertions(+), 36 deletions(-)

diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java
index ee69deb..138af98 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java
@@ -507,9 +507,8 @@ void sendTransferBlock(final DatanodeInfo[] targets,
           final StorageType[] targetStorageTypes,
           final Token<BlockTokenIdentifier> blockToken) throws IOException {
         //send the TRANSFER_BLOCK request
-        new Sender(out)
-            .transferBlock(block, blockToken, dfsClient.clientName, targets,
-                targetStorageTypes);
+        new Sender(out).transferBlock(block.getCurrentBlock(), blockToken,
+            dfsClient.clientName, targets, targetStorageTypes);
         out.flush();
         //ack
         BlockOpResponseProto transferResponse = BlockOpResponseProto
@@ -529,7 +528,7 @@ public void close() throws IOException {
     }
 
     private volatile boolean streamerClosed = false;
-    private volatile ExtendedBlock block; // its length is number of bytes acked
+    private final BlockToWrite block; // its length is number of bytes acked
     private Token<BlockTokenIdentifier> accessToken;
     private DataOutputStream blockStream;
     private DataInputStream blockReplyStream;
@@ -585,9 +584,11 @@ public DatanodeInfo load(DatanodeInfo key) throws Exception {
     /**
      * construction with tracing info
      */
-    private DataStreamer(HdfsFileStatus stat, EnumSet<AddBlockFlag> flags) {
+    private DataStreamer(HdfsFileStatus stat, EnumSet<AddBlockFlag> flags,
+        ExtendedBlock block) {
       isAppend = false;
       isLazyPersistFile = isLazyPersist(stat);
+      this.block = new BlockToWrite(block);
       stage = BlockConstructionStage.PIPELINE_SETUP_CREATE;
       addBlockFlags = flags;
     }
@@ -603,7 +604,7 @@ private DataStreamer(LocatedBlock lastBlock, HdfsFileStatus stat,
         int bytesPerChecksum) throws IOException {
       isAppend = true;
       stage = BlockConstructionStage.PIPELINE_SETUP_APPEND;
-      block = lastBlock.getBlock();
+      block = new BlockToWrite(lastBlock.getBlock());
       bytesSent = block.getNumBytes();
       accessToken = lastBlock.getBlockToken();
       isLazyPersistFile = isLazyPersist(stat);
@@ -1344,7 +1345,7 @@ private void addDatanode2ExistingPipeline() throws IOException {
         LocatedBlock lb;
         //get a new datanode
         lb = dfsClient.namenode.getAdditionalDatanode(
-            src, fileId, block, nodes, storageIDs,
+            src, fileId, block.getCurrentBlock(), nodes, storageIDs,
             exclude.toArray(new DatanodeInfo[exclude.size()]),
             1, dfsClient.clientName);
         // a new node was allocated by the namenode. Update nodes.
@@ -1522,7 +1523,8 @@ private boolean setupPipelineForAppendOrRecovery() throws IOException {
         }
 
         // get a new generation stamp and an access token
-        LocatedBlock lb = dfsClient.namenode.updateBlockForPipeline(block, dfsClient.clientName);
+        LocatedBlock lb = dfsClient.namenode.
+            updateBlockForPipeline(block.getCurrentBlock(), dfsClient.clientName);
         newGS = lb.getBlock().getGenerationStamp();
         accessToken = lb.getBlockToken();
         
@@ -1570,16 +1572,21 @@ private boolean setupPipelineForAppendOrRecovery() throws IOException {
 
       if (success) {
         // update pipeline at the namenode
-        ExtendedBlock newBlock = new ExtendedBlock(
-            block.getBlockPoolId(), block.getBlockId(), block.getNumBytes(), newGS);
-        dfsClient.namenode.updatePipeline(dfsClient.clientName, block, newBlock,
-            nodes, storageIDs);
-        // update client side generation stamp
-        block = newBlock;
+        final ExtendedBlock oldBlock = block.getCurrentBlock();
+        // the new GS has been propagated to all DN, it should be ok to update the
+        // local block state
+        block.setGenerationStamp(newGS);
+        dfsClient.namenode.updatePipeline(dfsClient.clientName, oldBlock,
+            block.getCurrentBlock(), nodes, storageIDs);
       }
       return false; // do not sleep, continue processing
     }
 
+    DatanodeInfo[] getExcludedNodes() {
+      return excludedNodes.getAllPresent(excludedNodes.asMap().keySet())
+          .keySet().toArray(new DatanodeInfo[0]);
+    }
+
     /**
      * Open a DataOutputStream to a DataNode so that it can be written to.
      * This happens when a file is created and each time a new block is allocated.
@@ -1592,22 +1599,17 @@ private LocatedBlock nextBlockOutputStream() throws IOException {
       StorageType[] storageTypes = null;
       int count = dfsClient.getConf().nBlockWriteRetry;
       boolean success = false;
-      ExtendedBlock oldBlock = block;
+      ExtendedBlock oldBlock = block.getCurrentBlock();
       do {
         hasError = false;
         lastException.set(null);
         errorIndex = -1;
         success = false;
 
-        long startTime = Time.now();
-        DatanodeInfo[] excluded =
-            excludedNodes.getAllPresent(excludedNodes.asMap().keySet())
-            .keySet()
-            .toArray(new DatanodeInfo[0]);
-        block = oldBlock;
-        lb = locateFollowingBlock(startTime,
-            excluded.length > 0 ? excluded : null);
-        block = lb.getBlock();
+        DatanodeInfo[] excluded = getExcludedNodes();
+        lb = locateFollowingBlock(
+            excluded.length > 0 ? excluded : null, oldBlock);
+        block.setCurrentBlock(lb.getBlock());
         block.setNumBytes(0);
         bytesSent = 0;
         accessToken = lb.getBlockToken();
@@ -1621,9 +1623,9 @@ private LocatedBlock nextBlockOutputStream() throws IOException {
 
         if (!success) {
           DFSClient.LOG.warn("Abandoning " + block);
-          dfsClient.namenode.abandonBlock(block, fileId, src,
-              dfsClient.clientName);
-          block = null;
+          dfsClient.namenode.abandonBlock(block.getCurrentBlock(),
+              fileId, src, dfsClient.clientName);
+          block.setCurrentBlock(null);
           DFSClient.LOG.warn("Excluding datanode " + nodes[errorIndex]);
           excludedNodes.put(nodes[errorIndex], nodes[errorIndex]);
         }
@@ -1686,7 +1688,7 @@ private boolean createBlockOutputStream(DatanodeInfo[] nodes,
 
           // We cannot change the block length in 'block' as it counts the number
           // of bytes ack'ed.
-          ExtendedBlock blockCopy = new ExtendedBlock(block);
+          ExtendedBlock blockCopy = block.getCurrentBlock();
           blockCopy.setNumBytes(blockSize);
 
           boolean[] targetPinnings = getPinnings(nodes);
@@ -1801,8 +1803,8 @@ private boolean createBlockOutputStream(DatanodeInfo[] nodes,
       }
     }
 
-    private LocatedBlock locateFollowingBlock(long start,
-        DatanodeInfo[] excludedNodes)  throws IOException {
+    protected LocatedBlock locateFollowingBlock(DatanodeInfo[] excluded,
+        ExtendedBlock oldBlock) throws IOException {
       int retries = dfsClient.getConf().nBlockWriteLocateFollowingRetry;
       long sleeptime = 400;
       while (true) {
@@ -1810,7 +1812,7 @@ private LocatedBlock locateFollowingBlock(long start,
         while (true) {
           try {
             return dfsClient.namenode.addBlock(src, dfsClient.clientName,
-                block, excludedNodes, fileId, favoredNodes, addBlockFlags);
+                oldBlock, excluded, fileId, favoredNodes, addBlockFlags);
           } catch (RemoteException e) {
             IOException ue = 
               e.unwrapRemoteException(FileNotFoundException.class,
@@ -1888,7 +1890,7 @@ private void backOffIfNecessary() throws InterruptedException {
     }
 
     ExtendedBlock getBlock() {
-      return block;
+      return block.getCurrentBlock();
     }
 
     DatanodeInfo[] getNodes() {
@@ -1904,6 +1906,42 @@ private void setLastException(IOException e) {
     }
   }
 
+  static class BlockToWrite {
+    private ExtendedBlock currentBlock;
+
+    BlockToWrite(ExtendedBlock block) {
+      setCurrentBlock(block);
+    }
+
+    synchronized ExtendedBlock getCurrentBlock() {
+      return currentBlock == null ? null : new ExtendedBlock(currentBlock);
+    }
+
+    synchronized long getNumBytes() {
+      return currentBlock == null ? 0 : currentBlock.getNumBytes();
+    }
+
+    synchronized void setCurrentBlock(ExtendedBlock block) {
+      currentBlock = (block == null || block.getLocalBlock() == null) ?
+          null : new ExtendedBlock(block);
+    }
+
+    synchronized void setNumBytes(long numBytes) {
+      assert currentBlock != null;
+      currentBlock.setNumBytes(numBytes);
+    }
+
+    synchronized void setGenerationStamp(long generationStamp) {
+      assert currentBlock != null;
+      currentBlock.setGenerationStamp(generationStamp);
+    }
+
+    @Override
+    public synchronized String toString() {
+      return currentBlock == null ? "null" : currentBlock.toString();
+    }
+  }
+
   /**
    * Create a socket for a write pipeline
    * @param first the first datanode 
@@ -2022,7 +2060,7 @@ private DFSOutputStream(DFSClient dfsClient, String src, HdfsFileStatus stat,
 
     computePacketChunkSize(dfsClient.getConf().writePacketSize, bytesPerChecksum);
 
-    streamer = new DataStreamer(stat, this.addBlockFlags);
+    streamer = new DataStreamer(stat, this.addBlockFlags, null);
     if (favoredNodes != null && favoredNodes.length != 0) {
       streamer.setFavoredNodes(favoredNodes);
     }
@@ -2098,7 +2136,7 @@ private DFSOutputStream(DFSClient dfsClient, String src,
       streamer = new DataStreamer(lastBlock, stat, bytesPerChecksum);
     } else {
       computePacketChunkSize(dfsClient.getConf().writePacketSize, bytesPerChecksum);
-      streamer = new DataStreamer(stat, this.addBlockFlags);
+      streamer = new DataStreamer(stat, this.addBlockFlags, null);
     }
     this.fileEncryptionInfo = stat.getFileEncryptionInfo();
   }
@@ -2406,8 +2444,11 @@ private void flushOrSync(boolean isSync, EnumSet<SyncFlag> syncFlags)
       // update the block length first time irrespective of flag
       if (updateLength || persistBlocks.get()) {
         synchronized (this) {
-          if (streamer != null && streamer.block != null) {
-            lastBlockLength = streamer.block.getNumBytes();
+          if (streamer != null && !streamer.streamerClosed) {
+            final ExtendedBlock block = streamer.getBlock();
+            if (block != null) {
+              lastBlockLength = block.getNumBytes();
+            }
           }
         }
       }
-- 
1.7.9.5

