From 16a6f60ceb099fd5d8959b65c0a714e810c9fc7a Mon Sep 17 00:00:00 2001
From: Colin Patrick Mccabe <cmccabe@cloudera.com>
Date: Mon, 30 Mar 2015 10:46:21 -0700
Subject: [PATCH 1490/2748] HDFS-7261. storageMap is accessed without
 synchronization in
 DatanodeDescriptor#updateHeartbeatState() (Brahma
 Reddy Battula via Colin P. McCabe)

(cherry picked from commit 1feb9569f366a29ecb43592d71ee21023162c18f)
(cherry picked from commit 02ed22cd2db7b5ff4d6e3d5be2a662973e5d3759)

Change-Id: Ibdee50ad6eb31246efd9e95180288b3bf63e0a3f
---
 .../server/blockmanagement/DatanodeDescriptor.java |   29 ++++++++++++--------
 1 file changed, 17 insertions(+), 12 deletions(-)

diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeDescriptor.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeDescriptor.java
index 9bc2684..8346108 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeDescriptor.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeDescriptor.java
@@ -450,8 +450,10 @@ public void updateHeartbeatState(StorageReport[] reports, long cacheCapacity,
     if (checkFailedStorages) {
       LOG.info("Number of failed storage changes from "
           + this.volumeFailures + " to " + volFailures);
-      failedStorageInfos = new HashSet<DatanodeStorageInfo>(
-          storageMap.values());
+      synchronized (storageMap) {
+        failedStorageInfos =
+            new HashSet<DatanodeStorageInfo>(storageMap.values());
+      }
     }
 
     setCacheCapacity(cacheCapacity);
@@ -482,8 +484,11 @@ public void updateHeartbeatState(StorageReport[] reports, long cacheCapacity,
     if (checkFailedStorages) {
       updateFailedStorage(failedStorageInfos);
     }
-
-    if (storageMap.size() != reports.length) {
+    long storageMapSize;
+    synchronized (storageMap) {
+      storageMapSize = storageMap.size();
+    }
+    if (storageMapSize != reports.length) {
       pruneStorageMap(reports);
     }
   }
@@ -493,14 +498,14 @@ public void updateHeartbeatState(StorageReport[] reports, long cacheCapacity,
    * as long as they have associated block replicas.
    */
   private void pruneStorageMap(final StorageReport[] reports) {
-    if (LOG.isDebugEnabled()) {
-      LOG.debug("Number of storages reported in heartbeat=" + reports.length +
-                    "; Number of storages in storageMap=" + storageMap.size());
-    }
+    synchronized (storageMap) {
+      if (LOG.isDebugEnabled()) {
+        LOG.debug("Number of storages reported in heartbeat=" + reports.length
+            + "; Number of storages in storageMap=" + storageMap.size());
+      }
 
-    HashMap<String, DatanodeStorageInfo> excessStorages;
+      HashMap<String, DatanodeStorageInfo> excessStorages;
 
-    synchronized (storageMap) {
       // Init excessStorages with all known storages.
       excessStorages = new HashMap<String, DatanodeStorageInfo>(storageMap);
 
@@ -517,8 +522,8 @@ private void pruneStorageMap(final StorageReport[] reports) {
           LOG.info("Removed storage " + storageInfo + " from DataNode" + this);
         } else if (LOG.isDebugEnabled()) {
           // This can occur until all block reports are received.
-          LOG.debug("Deferring removal of stale storage " + storageInfo +
-                        " with " + storageInfo.numBlocks() + " blocks");
+          LOG.debug("Deferring removal of stale storage " + storageInfo
+              + " with " + storageInfo.numBlocks() + " blocks");
         }
       }
     }
-- 
1.7.9.5

